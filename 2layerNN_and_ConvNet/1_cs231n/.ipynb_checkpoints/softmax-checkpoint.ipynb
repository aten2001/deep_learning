{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise guides you through the process of classifying images using a Softmax classifier. As part of this you will:\n",
    "\n",
    "- Implement a fully vectorized loss function for the Softmax classifier\n",
    "- Calculate the analytical gradient using vectorized code\n",
    "- Tune hyperparameters on a validation set\n",
    "- Optimize the loss function with Stochastic Gradient Descent (SGD)\n",
    "- Visualize the learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# start-up code! \n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, validation and testing sets have been created as \n",
      " X_i and y_i where i=train,val,test\n",
      "Train data shape:  (3073, 49000)\n",
      "Train labels shape:  (49000,)\n",
      "Val data shape:  (3073, 1000)\n",
      "Val labels shape:  (1000,)\n",
      "Test data shape:  (3073, 1000)\n",
      "Test labels shape:  (1000,)\n",
      "[6 9 9 ... 4 9 3]\n"
     ]
    }
   ],
   "source": [
    "from load_cifar10_tvt import load_cifar10_train_val\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10_train_val()\n",
    "print(\"Train data shape: \", X_train.shape)\n",
    "print(\"Train labels shape: \", y_train.shape)\n",
    "print(\"Val data shape: \", X_val.shape)\n",
    "print(\"Val labels shape: \", y_val.shape)\n",
    "print(\"Test data shape: \", X_test.shape)\n",
    "print(\"Test labels shape: \", y_test.shape)\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for this section is to be written in `cs231n/classifiers/softmax.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized loss: 2.352846e+00 computed in 0.250325s\n",
      "loss: 2.352846\n",
      "sanity check: 2.302585\n",
      "numerical: 0.315869 analytic: 0.315869, relative error: 1.805354e-07\n",
      "numerical: 1.545552 analytic: 1.545552, relative error: 4.668133e-09\n",
      "numerical: -1.745623 analytic: -1.745623, relative error: 1.798368e-08\n",
      "numerical: 2.584513 analytic: 2.584513, relative error: 1.378028e-08\n",
      "numerical: -1.372016 analytic: -1.372016, relative error: 1.483824e-08\n",
      "numerical: -0.646419 analytic: -0.646419, relative error: 2.072242e-08\n",
      "numerical: 1.674195 analytic: 1.674195, relative error: 8.602288e-09\n",
      "numerical: -0.963872 analytic: -0.963872, relative error: 2.329159e-08\n",
      "numerical: 2.013876 analytic: 2.013876, relative error: 4.318892e-08\n",
      "numerical: -1.663327 analytic: -1.663327, relative error: 1.237358e-08\n"
     ]
    }
   ],
   "source": [
    "# Now, implement the vectorized version in softmax_loss_vectorized.\n",
    "\n",
    "import time\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "\n",
    "# gradient check.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "\n",
    "W = np.random.randn(10, 3073) * 0.0001\n",
    "\n",
    "tic = time.time()\n",
    "loss, grad = softmax_loss_vectorized(W, X_train, y_train, 0.00001)\n",
    "toc = time.time()\n",
    "print(\"vectorized loss: %e computed in %fs\" % (loss, toc - tic))\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print(\"loss: %f\" % loss)\n",
    "print(\"sanity check: %f\" % (-np.log(0.1)))\n",
    "\n",
    "f = lambda w: softmax_loss_vectorized(w, X_train, y_train, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for this section is to be written in`cs231n/classifiers/linear_classifier.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "(200,)\n",
      "(200,)\n",
      "(200,)\n",
      "(200,)\n",
      "(200,)\n",
      "(200,)\n",
      "(200,)\n",
      "(200,)\n",
      "(200,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss value')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now that efficient implementations to calculate loss function and gradient of the softmax are ready,\n",
    "# use it to train the classifier on the cifar-10 data\n",
    "# Complete the `train` function in cs231n/classifiers/linear_classifier.py\n",
    "\n",
    "from cs231n.classifiers.linear_classifier import Softmax\n",
    "\n",
    "classifier = Softmax()\n",
    "loss_hist = classifier.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    learning_rate=1e-3,\n",
    "    reg=1e-5,\n",
    "    num_iters=10,\n",
    "    batch_size=200,\n",
    "    verbose=False,\n",
    ")\n",
    "# Plot loss vs. iterations\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.ylabel(\"Loss value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Complete the `predict` function in cs231n/classifiers/linear_classifier.py\n",
    "# Evaluate on test set\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print(\"softmax on raw pixels final test set accuracy: %f\" % (test_accuracy,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = classifier.W[:, :-1]  # strip out the bias\n",
    "w = w.reshape(10, 32, 32, 3)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = [\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype(\"uint8\"))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
